{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "latent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbTXmo51YuqtZG2xj1PeyN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimpapac/CNN_ANN_LSH_EMD_Clustering/blob/main/latent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BWGe2vaDfvH",
        "outputId": "df27baeb-6369-4131-a86c-3425d7ab83c4"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaOvu8b4GFDY",
        "outputId": "614ae34f-7e89-4281-fbac-85758a529f65"
      },
      "source": [
        "import os\r\n",
        "import sys\r\n",
        "import math\r\n",
        "import struct as st\r\n",
        "import numpy as np\r\n",
        "import keras\r\n",
        "import sklearn\r\n",
        "import matplotlib.pyplot as plt #Graph\r\n",
        "from keras import backend as k\r\n",
        "from keras import layers, optimizers, losses, metrics\r\n",
        "from keras.models import Model #ANN architecture\r\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Dropout, Flatten, Reshape\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "\r\n",
        "# dataset = \"\"\r\n",
        "# flag = 1\r\n",
        "\r\n",
        "# if len(sys.ar4ge: $python autoencoder.py -d <dataset>\")\r\n",
        "# \tsys.exit()\r\n",
        "\r\n",
        "# for i in range(len(sys.argv)):\r\n",
        "# \tif sys.argv[i] == \"-d\":\r\n",
        "# \t    dataset = sys.argv[i+1]\r\n",
        "# \t    flag-=1\r\n",
        "\r\n",
        "# if flag!=0:\r\n",
        "# \tprint(\"Usage: $python autoencoder.py -d <dataset>\")\r\n",
        "# \tsys.exit()\r\n",
        "\r\n",
        "\r\n",
        "# print(\"i: \", dataset)\r\n",
        "\r\n",
        "\r\n",
        "# trainfilename = {'images' : './data/train-images.idx3-ubyte' ,'labels' : './data/train-labels.idx1-ubyte'}\r\n",
        "# testfilename = {'images' : './data/t10k-images.idx3-ubyte' ,'labels' : './data/t10k-labels.idx1-ubyte'}\r\n",
        "\r\n",
        "with open('/content/drive/My Drive/project3/data/train-images.idx3-ubyte','rb') as f:\r\n",
        "# with open(dataset,'rb') as f:\r\n",
        "    magic, size = st.unpack(\">II\", f.read(8))\r\n",
        "    nrows, ncols = st.unpack(\">II\", f.read(8))\r\n",
        "    train_images = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\r\n",
        "    train_images = train_images.reshape((size, nrows, ncols))\r\n",
        "print(train_images.shape)\r\n",
        "\r\n",
        "\r\n",
        "with open('/content/drive/My Drive/project3/data/train-labels.idx1-ubyte','rb') as f:\r\n",
        "    magic, size = st.unpack(\">II\", f.read(8))\r\n",
        "    # nrows, ncols = st.unpack(\">II\", f.read(8))\r\n",
        "    train_labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\r\n",
        "    train_labels = train_labels.reshape((size))\r\n",
        "print(train_labels.shape)\r\n",
        "\r\n",
        "\r\n",
        "with open('/content/drive/My Drive/project3/data/t10k-images.idx3-ubyte','rb') as f:\r\n",
        "    magic, size = st.unpack(\">II\", f.read(8))\r\n",
        "    nrows, ncols = st.unpack(\">II\", f.read(8))\r\n",
        "    test_images = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\r\n",
        "    test_images = test_images.reshape((size, nrows, ncols))\r\n",
        "print(test_images.shape)\r\n",
        "\r\n",
        "with open('/content/drive/My Drive/project3/data/t10k-labels.idx1-ubyte','rb') as f:\r\n",
        "    magic, size = st.unpack(\">II\", f.read(8))\r\n",
        "    # nrows, ncols = st.unpack(\">II\", f.read(8))\r\n",
        "    test_labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\r\n",
        "    test_labels = test_labels.reshape((size))\r\n",
        "print(test_labels.shape)\r\n",
        "\r\n",
        "train_images = (train_images/255) #normalize\r\n",
        "test_images = (test_images/255) #normalize\r\n",
        "\r\n",
        "train_images = np.reshape(train_images, (len(train_images), 28, 28, 1))\r\n",
        "\r\n",
        "print(\"train_images\", train_images.shape) #60.000 rows\r\n",
        "\r\n",
        "loss = []\r\n",
        "val_loss = []\r\n",
        "text_values = []\r\n",
        "\r\n",
        "usr_input = '1'\r\n",
        "while (usr_input == '1'):\r\n",
        "    \r\n",
        "    #input from user\r\n",
        "    layers, batch_size, epochs = input(\"#layers batch_size epochs: \").split()\r\n",
        "\r\n",
        "    #accept only digits\r\n",
        "    if  (not(layers.isdigit()) or not(batch_size.isdigit()) or not(epochs.isdigit())):\r\n",
        "        print(\"wrong input\")\r\n",
        "        sys.exit()\r\n",
        "    print()\r\n",
        "\r\n",
        "    layersNum = int(layers)\r\n",
        "    batchSize = int(batch_size)\r\n",
        "    epochsNum = int(epochs)\r\n",
        "    input_img = Input(shape=(28, 28, 1)) \r\n",
        "\r\n",
        "    #input = 28 x 28 x 1 (wide and thin)\r\n",
        "    x = input_img\r\n",
        "    pooling_counter = 0\r\n",
        "    latent = 0\r\n",
        "\r\n",
        "    #keep every layer's info to use it in the decoder (mirror)\r\n",
        "    layer_info = []\r\n",
        "\r\n",
        "    for i in range(0, layersNum):\r\n",
        "\r\n",
        "        # Ask for number of filters ,filtersize and if you want pooling\r\n",
        "        filter_flag = 1\r\n",
        "        while filter_flag :\r\n",
        "            filters, filter_size  = input(\"#total_filters #filter_size : \").split()\r\n",
        "            #accept only digits\r\n",
        "            if  (not(filters.isdigit()) or not(filter_size.isdigit())):\r\n",
        "                print(\"please enter ints\")\r\n",
        "            else:\r\n",
        "                filtersNum = int(filters)\r\n",
        "                filterSize = int(filter_size)\r\n",
        "                filter_flag = 0\r\n",
        "\r\n",
        "        pooling_flag = 1\r\n",
        "        pool = 'n'\r\n",
        "        while pooling_flag :\r\n",
        "            # pool = input(\"Do you want to add pooling to this layer? [Y/n]\")\r\n",
        "            pool = 'Y'\r\n",
        "            # Accept only Y or n\r\n",
        "            if ( pool != 'Y' and pool != 'n' ) :\r\n",
        "                print(\"please enter Y (yes) or n (no)\")\r\n",
        "            else :\r\n",
        "                if pooling_counter == 2 and pool == 'Y' :\r\n",
        "                    print(\"already added max amount of pooling\")\r\n",
        "                    pool = 'n'\r\n",
        "                else :\r\n",
        "                    if( pool == 'Y' ) :\r\n",
        "                        pooling_counter += 1\r\n",
        "                pooling_flag = 0\r\n",
        "\r\n",
        "        # For each layer of the encoder keep the parameters used so we can use them on the right layers on the decoder\r\n",
        "        info = [filtersNum,filterSize,pool]\r\n",
        "        layer_info.insert(0,info)\r\n",
        "\r\n",
        "        #encoder\r\n",
        "        x = Conv2D(filtersNum, (filterSize, filterSize), activation='relu', padding='same' )(x)\r\n",
        "        x = BatchNormalization()(x)\r\n",
        "        if pool == 'Y' : #only in 2 first layers\r\n",
        "            x = MaxPooling2D(pool_size=(2, 2))(x) \r\n",
        "\r\n",
        "    latent = int(input(\"Please give latent dimension : \"))\r\n",
        "\r\n",
        "    shape_before_flattening = k.int_shape(x)[1:]\r\n",
        "    flat = Flatten()(x)\r\n",
        "    lat = Dense(latent)(flat)\r\n",
        "    \r\n",
        "\r\n",
        "    # encoder = Model(input_img, lat)\r\n",
        "\r\n",
        "    #decoder\r\n",
        "    x = Dense(np.prod(shape_before_flattening))(lat)\r\n",
        "    x = Reshape(shape_before_flattening)(x)\r\n",
        "\r\n",
        "    for i in range(0,layersNum):\r\n",
        "\r\n",
        "        info = layer_info[i]\r\n",
        "        filtersNum = info[0]\r\n",
        "        filterSize = info[1]\r\n",
        "        pool = info[2]\r\n",
        "\r\n",
        "        #decoder\r\n",
        "        x = Conv2D(filtersNum, (filterSize, filterSize), activation='relu', padding='same')(x)\r\n",
        "        x = BatchNormalization()(x)\r\n",
        "        if pool == 'Y' :\r\n",
        "            x = UpSampling2D((2,2))(x) \r\n",
        "\r\n",
        "\r\n",
        "    decoded = Conv2D(1, (filterSize, filterSize), activation='sigmoid', padding='same')(x) # 28 x 28 x 1\r\n",
        "\r\n",
        "    autoencoder = Model(input_img, decoded)\r\n",
        "    autoencoder.compile(loss = 'mean_squared_error', optimizer = optimizers.RMSprop())\r\n",
        "\r\n",
        "    autoencoder.summary()\r\n",
        "\r\n",
        "    train_X,valid_X,train_ground,valid_ground = train_test_split(\r\n",
        "        train_images,\r\n",
        "        train_images,\r\n",
        "        test_size=0.2,\r\n",
        "        random_state=13\r\n",
        "        )\r\n",
        "\r\n",
        "    #fit model with given params\r\n",
        "    autoencoder_train = autoencoder.fit(train_X, train_ground, \r\n",
        "        batch_size=batchSize,\r\n",
        "        epochs=epochsNum,\r\n",
        "        verbose=1,\r\n",
        "        validation_data=(valid_X, valid_X)\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "    encoder = Model(input_img, lat)\r\n",
        "    compressed = encoder.predict(train_images)\r\n",
        "\r\n",
        "    compressed = compressed.astype(float)\r\n",
        "    flat_list = [item for sublist in compressed for item in sublist]\r\n",
        "    max1 = max(flat_list)\r\n",
        "    min1 = min(flat_list)\r\n",
        "\r\n",
        "    for i in range(len(compressed)):\r\n",
        "        for j in range(latent):\r\n",
        "            compressed[i][j] = (compressed[i][j] - min1 ) * (25500 // (max1 - min1))\r\n",
        "        # print(compressed[i])\r\n",
        "\r\n",
        "    compressed = compressed.astype(int)\r\n",
        "\r\n",
        "    output_file = open('/content/drive/My Drive/project3/output', 'wb')\r\n",
        "    output_file.write((1312).to_bytes(4, 'big'))\r\n",
        "    output_file.write((len(train_images)).to_bytes(4, 'big'))\r\n",
        "    output_file.write((1).to_bytes(4, 'big'))\r\n",
        "    output_file.write((latent).to_bytes(4, 'big'))\r\n",
        "    for i in compressed:\r\n",
        "        for j in i:\r\n",
        "            output_file.write(j.item().to_bytes(2, 'big'))\r\n",
        "    output_file.close()\r\n",
        "\r\n",
        "    with open('/content/drive/My Drive/project3/output', 'rb') as file:\r\n",
        "        print(int.from_bytes(file.read(4), byteorder='big'))\r\n",
        "        print(int.from_bytes(file.read(4), byteorder='big'))\r\n",
        "        print(int.from_bytes(file.read(4), byteorder='big'))\r\n",
        "        print(int.from_bytes(file.read(4), byteorder='big'))\r\n",
        "        for i in range(10):\r\n",
        "            print(int.from_bytes(file.read(2), byteorder='big'))\r\n",
        "\r\n",
        "\r\n",
        "    # print(autoencoder_train.history.keys())\r\n",
        "    loss.append(autoencoder_train.history['loss'][-1])\r\n",
        "    val_loss.append(autoencoder_train.history['val_loss'][-1])\r\n",
        "    \r\n",
        "    str_text = \"\"\r\n",
        "    str_num = \"\"\r\n",
        "    str_size = \"\"\r\n",
        "    for i in range(layersNum-1,-1,-1):\r\n",
        "        info = layer_info[i]\r\n",
        "        filtersNum = info[0]\r\n",
        "        filterSize = info[1]\r\n",
        "        str_num = str_num + \" \" + str(filtersNum)\r\n",
        "        str_size = str_size + \" \" + str(filterSize)\r\n",
        "    strr = \"layers: %d\\n fNum: %s\\n fSize: %s\\n batch_size: %d\\nepochs: %d\\n\" % (layersNum, str_num, str_size, batchSize, epochsNum)\r\n",
        "    text_values.append(strr)    \r\n",
        "    # print(autoencoder_train.history['loss'])\r\n",
        "    # print(autoencoder_train.history['val_loss'][-1])\r\n",
        "\r\n",
        "    print(\"Options:\")\r\n",
        "    print(\"Press 1 to repeat with different hyperparameters\")\r\n",
        "    print(\"Press 2 to plot\")\r\n",
        "    print(\"Press 3 to save the model\")\r\n",
        "    usr_input = input(\"Input: \")\r\n",
        "\r\n",
        "    if usr_input == '2':\r\n",
        "        print(\"plot\")\r\n",
        "\r\n",
        "        decoded_imgs = autoencoder.predict(test_images)\r\n",
        "        n = 10\r\n",
        "\r\n",
        "        plt.figure(figsize=(20, 4))\r\n",
        "        for i in range(n):\r\n",
        "            # display original\r\n",
        "            ax = plt.subplot(2, n, i + 1)\r\n",
        "            plt.imshow(test_images[i].reshape(28, 28))\r\n",
        "            plt.gray()\r\n",
        "            ax.get_xaxis().set_visible(False)\r\n",
        "            ax.get_yaxis().set_visible(False)\r\n",
        "\r\n",
        "            # display reconstruction\r\n",
        "            ax = plt.subplot(2, n, i+1+n)\r\n",
        "            plt.imshow(decoded_imgs[i].reshape(28, 28))\r\n",
        "            plt.gray()\r\n",
        "            ax.get_xaxis().set_visible(False)\r\n",
        "            ax.get_yaxis().set_visible(False)\r\n",
        "        plt.show()\r\n",
        "\r\n",
        "        plt.plot(autoencoder_train.history['loss'])\r\n",
        "        plt.plot(autoencoder_train.history['val_loss'])\r\n",
        "        plt.title('model loss')\r\n",
        "        plt.ylabel('loss')\r\n",
        "        plt.xlabel('epoch')\r\n",
        "        plt.legend(['train', 'val'], loc='upper left')\r\n",
        "        plt.show()\r\n",
        "\r\n",
        "\r\n",
        "        x_values = np.arange(0, len(text_values), 1)\r\n",
        "\r\n",
        "        plt.figure(figsize=(len(text_values)*1.5, 4))\r\n",
        "        plt.plot(loss)\r\n",
        "        plt.plot(val_loss)\r\n",
        "        plt.title('model loss')\r\n",
        "        plt.ylabel('loss')\r\n",
        "        plt.xlabel('hyperparameters')\r\n",
        "        plt.legend(['train', 'val'], loc='upper left')\r\n",
        "        plt.xticks(x_values, text_values)\r\n",
        "        plt.show()\r\n",
        "\r\n",
        "    elif usr_input == '3':\r\n",
        "        print(\"save model\")\r\n",
        "\r\n",
        "        autoencoder.save('autoencoder.h5')\r\n",
        "\r\n",
        "        # save encoder part for classifier.py\r\n",
        "        path = input(\"Give the path in which you want to save the model: \")\r\n",
        "        print(path)\r\n",
        "        if not os.path.exists(path):\r\n",
        "            print('save directories...', flush=True)\r\n",
        "            os.makedirs(path)\r\n",
        "        encoder.save(path + '/latent.h5')\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "train_images (60000, 28, 28, 1)\n",
            "#layers batch_size epochs: 2 512 1\n",
            "\n",
            "#total_filters #filter_size : 16 3\n",
            "#total_filters #filter_size : 32 3\n",
            "Please give latent dimension : 10\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 28, 28, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 28, 28, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 14, 14, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1568)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                15690     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1568)              17248     \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 7, 7, 32)          9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 7, 7, 32)          128       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 14, 14, 16)        4624      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 14, 14, 16)        64        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2 (None, 28, 28, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 28, 28, 1)         145       \n",
            "=================================================================\n",
            "Total params: 52,139\n",
            "Trainable params: 51,947\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n",
            "94/94 [==============================] - 3s 18ms/step - loss: 0.1337 - val_loss: 0.0943\n",
            "1312\n",
            "60000\n",
            "1\n",
            "10\n",
            "10767\n",
            "16385\n",
            "11884\n",
            "12800\n",
            "9287\n",
            "12918\n",
            "8836\n",
            "19465\n",
            "15271\n",
            "13529\n",
            "Options:\n",
            "Press 1 to repeat with different hyperparameters\n",
            "Press 2 to plot\n",
            "Press 3 to save the model\n",
            "Input: 21\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}